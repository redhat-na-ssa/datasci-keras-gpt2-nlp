{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# More on the GPT-2 model from KerasNLP\n",
    "\n",
    "Next up, we will actually fine-tune the model to update its parameters, but before we do, let's take a look at the full set of tools we have to for working with for [GPT2](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/).\n",
    "\n",
    "The code of GPT2 can be found [here](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/). Conceptually the GPT2CausalLM can be hierarchically broken down into several modules in KerasNLP, all of which have a from_preset() function that loads a pretrained model:\n",
    "\n",
    "[keras_nlp.models.GPT2Tokenizer](https://keras.io/api/keras_nlp/models/gpt2/gpt2_tokenizer#gpt2tokenizer-class): The tokenizer used by GPT2 model, which is a [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt).\n",
    "[keras_nlp.models.GPT2CausalLMPreprocessor](https://keras.io/api/keras_nlp/models/gpt2/gpt2_causal_lm_preprocessor#gpt2causallmpreprocessor-class): the preprocessor used by GPT2 causal LM training. It does the tokenization along with other preprocessing works such as creating the label and appending the end token.\n",
    "[keras_nlp.models.GPT2Backbone](https://keras.io/api/keras_nlp/models/gpt2/gpt2_backbone#gpt2backbone-class): the GPT2 model, which is a stack of [keras_nlp.layers.TransformerDecoder](https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder#transformerdecoder-class). This is usually just referred as GPT2.\n",
    "[keras_nlp.models.GPT2CausalLM](https://keras.io/api/keras_nlp/models/gpt2/gpt2_causal_lm#gpt2causallm-class): wraps GPT2Backbone, it multiplies the output of GPT2Backbone by embedding matrix to generate logits over vocab tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pip -U -q\n",
    "%pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda_malloc_async has fewer fragmentation issues than the default BFC memory allocator - https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tf_gpu_allocator\n",
    "\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "print(os.getenv(\"TF_GPU_ALLOCATOR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model previously trained\n",
    "\n",
    "|Preset name|Compile Time|\n",
    "|-----------|-----------|\n",
    "|gpt2_base_en\t| \t|\n",
    "|gpt2_medium_en\t| 6m 36.8s\t|\n",
    "|gpt2_large_en\t|\t|\n",
    "|gpt2_extra_large_en\t|\t|\n",
    "|gpt2_base_en_cnn_dailymail\t|\t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this load takes about \n",
    "gpt2_lm = keras.models.load_model(\"../models/gpt2_lm.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Finetune on custom dataset\n",
    "\n",
    "Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one step further to finetune the model so that it generates text in a specific style, short or long, strict or casual. In this tutorial, we will use reddit dataset for example.\n",
    "\n",
    "Here is a list of of some [abstract text summarizations from tf datasets](https://www.tensorflow.org/datasets/catalog/overview#abstractive_text_summarization)\n",
    "\n",
    "|Dataset|Description|Download Size|Download Time|\n",
    "|-------|----------|----------|----------|\n",
    "|aeslc|A collection of email messages of employees in the Enron Corporation.|11.10M|    |\n",
    "|billsum| summarization of US Congressional and California state bills.|64.14M| |\n",
    "|reddit| preprocessed posts from the Reddit dataset consists of 3,848,330 posts with an average length of 270 words for content, and 28 words for the summary.|2.9G| |\n",
    "|reddit_tifu| Reddit dataset, where TIFU denotes the name of subbreddit /r/tifu.|639.54M|   |\n",
    "|scientific_papers|scientific papers datasets are obtained from ArXiv and PubMed OpenAccess repositories. |4.2G|    |\n",
    "\n",
    "*download speed from hotel WiFi at 25Mbps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ds = \"scientific_papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: you will want to update line 27 in your `devfile.yaml`           \n",
    "`memory: 48Gi # you will want at least 48Gi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "source_ds = tfds.load(custom_ds, split=\"train\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's take a look inside sample data from the reddit TensorFlow Dataset. There are two features:\n",
    "\n",
    "document: text of the post.\n",
    "title: the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for document, title in source_ds:\n",
    "    print(document.numpy())\n",
    "    print(title.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In our case, we are performing next word prediction in a language model, so we only need the 'document' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    source_ds.map(lambda document, _: document)\n",
    "    .batch(32)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now you can finetune the model using the familiar fit() function. Note that preprocessor will be automatically called inside fit method since GPT2CausalLM is a keras_nlp.models.Task instance.\n",
    "\n",
    "This step takes quite a bit of GPU memory and a long time if we were to train it all the way to a fully trained state. Here we just use part of the dataset for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "# Linearly decaying learning rate.\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-5,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After fine-tuning is finished, you can again generate text using the same generate() function. This time, the text will be closer to scientific papers writing style, and the generated length will be close to our preset length in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"Red hat is\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the fine-tuned GPT-2 model to object storage\n",
    "\n",
    "You can save the model in different formats depending on how you intend to serve the model. In short, this save will enable us to do early online experimentation with the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local storage\n",
    "filename = \"../models/gpt2_lm.keras\"\n",
    "\n",
    "gpt2_lm.save(filename)\n",
    "# gpt2_lm.save('../models/gpt2_lm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(model):\n",
    "    import onnx\n",
    "    import tf2onnx\n",
    "\n",
    "    export_filename = filename.replace('.keras', '.onnx')\n",
    "\n",
    "    proto, _ = tf2onnx.convert.from_keras(model)\n",
    "    onnx.save(proto, export_filename)\n",
    "\n",
    "convert_model_to_onnx(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to S3 Object Storage (Minio)\n",
    "\n",
    "Lets use the NVIDIA Triton model folder structure to store the saved models\n",
    "\n",
    "Triton model folder structure:\n",
    "\n",
    "```\n",
    "models (provide this dir as source / MODEL_REPOSITORY )\n",
    "└─ [ model name ]\n",
    "    └─ 1 (version)\n",
    "        └── model.savedmodel (we will use .keras)\n",
    "            ├── saved_model.pb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements\n",
    "\n",
    "%pip install -U boto3 python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming Minio is deployed, populate the environment variables\n",
    "\n",
    "!  echo \"AWS_S3_BUCKET=${AWS_S3_BUCKET:-models}\" > .env\n",
    "!  echo \"AWS_S3_ENDPOINT=${AWS_S3_ENDPOINT:-http://minio.minio.svc:9000}\" >> .env\n",
    "!  echo \"AWS_ACCESS_KEY_ID=$(oc -n minio extract secret/minio-root-user --keys=MINIO_ROOT_USER --to=-)\" >> .env\n",
    "!  echo \"AWS_SECRET_ACCESS_KEY=$(oc -n minio extract secret/minio-root-user --keys=MINIO_ROOT_PASSWORD --to=-)\" >> .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages\n",
    "\n",
    "import os, boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the model from local storage to S3\n",
    "\n",
    "local_path = \"../models\"\n",
    "remote_path = \"gpt2/2\"\n",
    "\n",
    "bucket = os.getenv(\"AWS_S3_BUCKET\", \"models\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=os.getenv(\"AWS_S3_ENDPOINT\", \"http://minio.minio.svc:9000\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\", \"minioadmin\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"minioadmin\"),\n",
    ")\n",
    "\n",
    "\n",
    "if bucket not in [bu[\"Name\"] for bu in s3.list_buckets()[\"Buckets\"]]:\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "\n",
    "\n",
    "def uploadDirectory(path, bucketname):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            print(f\"uploading: {file} to {bucket}/{remote_path}\")\n",
    "            s3.upload_file(\n",
    "                os.path.join(root, file), bucketname, f\"{remote_path}/{file}\"\n",
    "            )\n",
    "            print(\"[ok]\")\n",
    "\n",
    "\n",
    "uploadDirectory(path=local_path, bucketname=bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Clear All Outputs and close the notebook before running 03_ notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
