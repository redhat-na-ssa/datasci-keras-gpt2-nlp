{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Before we begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this tutorial:\n",
    "1. You will learn to use [KerasNLP](https://keras.io/keras_nlp/) to load a pre-trained Large Language Model (LLM) - [GPT-2 model](https://openai.com/research/better-language-models) (originally invented by OpenAI)\n",
    "2. finetune it to a specific text style\n",
    "3. generate text based on users' input (also known as prompt). \n",
    "\n",
    "You will also learn how GPT2 adapts quickly to non-English languages, such as Chinese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Install KerasNLP, Choose Backend and Import Dependencies\n",
    "\n",
    "This examples uses [Keras Core](https://keras.io/keras_core/) to work in any of \"tensorflow\", \"jax\" or \"torch\". Support for Keras Core is baked into KerasNLP, simply change the \"KERAS_BACKEND\" environment variable to select the backend of your choice. We select the JAX backend below.\n",
    "\n",
    "Source tutorial: https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install pip -U -q\n",
    "%pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction to Generative Large Language Models (LLMs)\n",
    "\n",
    "Large language models (LLMs) are a type of machine learning models that are trained on a large corpus of text data to generate outputs for various natural language processing (NLP) tasks, such as text generation, question answering, and machine translation.\n",
    "\n",
    "Generative LLMs are typically based on deep learning neural networks, such as the [Transformer architecture](https://arxiv.org/abs/1706.03762) invented by Google researchers in 2017, and are trained on massive amounts of text data, often involving billions of words. These models, such as Google [LaMDA](https://blog.google/technology/ai/lamda/) and [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html), are trained with a large dataset from various data sources which allows them to generate output for many tasks. The core of Generative LLMs is predicting the next word in a sentence, often referred as Causal LM Pre-training. In this way LLMs can generate coherent text based on user prompts. For a more pedagogical discussion on language models, you can refer to the [Stanford CS324 LLM class](https://stanford-cs324.github.io/winter2022/lectures/introduction/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction to KerasNLP\n",
    "\n",
    "## Pre-trained LLMs\n",
    "Large Language Models are complex to build and expensive to train from scratch. Luckily there are pre-trained LLMs available for use right away. [KerasNLP](https://keras.io/keras_nlp/) provides a large number of pre-trained checkpoints that allow you to experiment with SOTA models without needing to train them yourself.\n",
    "\n",
    "KerasNLP is a natural language processing library that supports users through their entire development cycle. KerasNLP offers both pre-trained models and modularized building blocks, so developers could easily reuse pre-trained models or stack their own LLM.\n",
    "\n",
    "## KerasNLP\n",
    "\n",
    "Pre-trained models with generate() method, e.g., [keras_nlp.models.GPT2CausalLM](https://keras.io/api/keras_nlp/models/gpt2/gpt2_causal_lm#gpt2causallm-class) and [keras_nlp.models.OPTCausalLM](https://keras.io/api/keras_nlp/models/opt/opt_causal_lm#optcausallm-class).\n",
    "Sampler class that implements generation algorithms such as Top-K, Beam and contrastive search. These samplers can be used to generate text with custom models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load a pre-trained GPT-2 model and generate some text\n",
    "\n",
    "KerasNLP provides a number of pre-trained models, such as [Google Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) and [GPT-2](https://openai.com/research/better-language-models). You can see the list of models available in the [KerasNLP repository](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n",
    "\n",
    "It's very easy to load the GPT-2 model and interchange with other versions.\n",
    "\n",
    "|Preset name|\tParameters|\tDescription|Download Time| Model Size|\n",
    "|-----------|-----------|-----------|-----------|-----------|\n",
    "|gpt2_base_en\t|124.44M\t|12-layer GPT-2 model where case is maintained. Trained on WebText.| ~13.5s| 477M  |\n",
    "|gpt2_medium_en\t|354.82M\t|24-layer GPT-2 model where case is maintained. Trained on WebText.|  ~39s  | 1.4G  |\n",
    "|gpt2_large_en\t|774.03M\t|36-layer GPT-2 model where case is maintained. Trained on WebText.|    |   |\n",
    "|gpt2_extra_large_en\t|1.56B\t|48-layer GPT-2 model where case is maintained. Trained on WebText.|    |   |\n",
    "|gpt2_base_en_cnn_dailymail\t|124.44M\t|12-layer GPT-2 model where case is maintained. Finetuned on the CNN/DailyMail summarization dataset.|  |   |\n",
    "\n",
    "*download speed from hotel WiFi at 25 Mbps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and paste the model\n",
    "\n",
    "base_model = \"gpt2_medium_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To speed up training and generation, we use preprocessor of length 128\n",
    "# instead of full length 1024.\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    base_model,\n",
    "    sequence_length=128,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    base_model, preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once the model is loaded, you can use it to generate some text right away. Run the cells below to give it a try. It's as simple as calling a single function generate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can kubernetes run LLM training in the context of AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# We slightly altered the text to understand what the model thought of us?\n",
    "output = gpt2_lm.generate(prompt, max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Try another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How do I check if you are hallucinating in my jupyter notebook in the context of AI LLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(prompt, max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice how much faster the second call is. This is because the computational graph is [XLA compiled](https://www.tensorflow.org/xla) in the 1st run and re-used in the 2nd behind the scenes.\n",
    "\n",
    "The quality of the generated text looks OK, but we can improve it via fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the pre-trained GPT-2 model to storage\n",
    "\n",
    "You can save the model in different formats depending on how you intend to serve the model. In short, this save will enable us to do early online experimentation with the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local storage\n",
    "\n",
    "gpt2_lm.save(\"../models/gpt2_lm.keras\")\n",
    "# gpt2_lm.save('../models/gpt2_lm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to S3 Object Storage (Minio)\n",
    "\n",
    "Lets use the NVIDIA Triton model folder structure to store the saved models\n",
    "\n",
    "Triton model folder structure:\n",
    "```\n",
    "models (provide this dir as source / MODEL_REPOSITORY )\n",
    "└─ [ model name ]\n",
    "    └─ 1 (version)\n",
    "        └── model.savedmodel (we will use .keras)\n",
    "            ├── saved_model.pb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements\n",
    "\n",
    "%pip install -U boto3 python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming Minio is deployed, populate the environment variables\n",
    "\n",
    "!  echo \"AWS_S3_BUCKET=${AWS_S3_BUCKET:-models}\" > .env\n",
    "!  echo \"AWS_S3_ENDPOINT=${AWS_S3_ENDPOINT:-http://minio.minio.svc:9000}\" >> .env\n",
    "!  echo \"AWS_ACCESS_KEY_ID=$(oc -n minio extract secret/minio-root-user --keys=MINIO_ROOT_USER --to=-)\" >> .env\n",
    "!  echo \"AWS_SECRET_ACCESS_KEY=$(oc -n minio extract secret/minio-root-user --keys=MINIO_ROOT_PASSWORD --to=-)\" >> .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages\n",
    "\n",
    "import os, boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the models from local storage to S3\n",
    "\n",
    "local_path = \"../models\"\n",
    "remote_path = \"gpt2/1\"\n",
    "\n",
    "bucket = os.getenv(\"AWS_S3_BUCKET\", \"models\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=os.getenv(\"AWS_S3_ENDPOINT\", \"http://minio.minio.svc:9000\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\", \"minioadmin\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"minioadmin\"),\n",
    ")\n",
    "\n",
    "\n",
    "if bucket not in [bu[\"Name\"] for bu in s3.list_buckets()[\"Buckets\"]]:\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "\n",
    "\n",
    "def uploadDirectory(path, bucketname):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            print(f\"uploading: {file} to {bucket}/{remote_path}\")\n",
    "            s3.upload_file(\n",
    "                os.path.join(root, file), bucketname, f\"{remote_path}/{file}\"\n",
    "            )\n",
    "            print(\"[ok]\")\n",
    "\n",
    "\n",
    "uploadDirectory(path=local_path, bucketname=bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Clear All Outputs and close the notebook before running 02_ notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
