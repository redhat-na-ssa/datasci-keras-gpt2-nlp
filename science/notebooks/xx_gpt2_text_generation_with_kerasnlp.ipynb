{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT2 Text Generation with KerasNLP:\n",
    "\n",
    "- Before we begin\n",
    "- Install KerasNLP, Choose Backend and Import Dependencies\n",
    "- Introduction to Generative Large Language Models (LLMs)\n",
    "- Introduction to KerasNLP\n",
    "- Load a pre-trained GPT-2 model and generate some text\n",
    "- More on the GPT-2 model from KerasNLP\n",
    "- Finetune on Reddit dataset\n",
    "- Into the Sampling Method\n",
    "- Finetune on Chinese Poem Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Before we begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this tutorial:\n",
    "1. You will learn to use [KerasNLP](https://keras.io/keras_nlp/) to load a pre-trained Large Language Model (LLM) - [GPT-2 model](https://openai.com/research/better-language-models) (originally invented by OpenAI)\n",
    "2. finetune it to a specific text style\n",
    "3. generate text based on users' input (also known as prompt). You will also learn how GPT2 adapts quickly to non-English languages, such as Chinese.\n",
    "\n",
    "This tutorial is demonstrated on OpenShift with advanced features, including:\n",
    "- Dev Spaces IDE with Jupyter\n",
    "- shareable notebook images\n",
    "- node autoscaling\n",
    "- distributed training on GPUs\n",
    "- multi-instance GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Install KerasNLP, Choose Backend and Import Dependencies\n",
    "\n",
    "This examples uses [Keras Core](https://keras.io/keras_core/) to work in any of \"tensorflow\", \"jax\" or \"torch\". Support for Keras Core is baked into KerasNLP, simply change the \"KERAS_BACKEND\" environment variable to select the backend of your choice. We select the JAX backend below.\n",
    "\n",
    "Source tutorial: https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pip -U -q\n",
    "%pip install tensorflow~=2.13.1 keras-nlp==0.6.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 20:31:44.766824: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-16 20:31:44.821807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 20:31:45.627566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction to Generative Large Language Models (LLMs)\n",
    "\n",
    "Large language models (LLMs) are a type of machine learning models that are trained on a large corpus of text data to generate outputs for various natural language processing (NLP) tasks, such as text generation, question answering, and machine translation.\n",
    "\n",
    "Generative LLMs are typically based on deep learning neural networks, such as the [Transformer architecture](https://arxiv.org/abs/1706.03762) invented by Google researchers in 2017, and are trained on massive amounts of text data, often involving billions of words. These models, such as Google [LaMDA](https://blog.google/technology/ai/lamda/) and [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html), are trained with a large dataset from various data sources which allows them to generate output for many tasks. The core of Generative LLMs is predicting the next word in a sentence, often referred as Causal LM Pretraining. In this way LLMs can generate coherent text based on user prompts. For a more pedagogical discussion on language models, you can refer to the [Stanford CS324 LLM class](https://stanford-cs324.github.io/winter2022/lectures/introduction/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction to KerasNLP\n",
    "\n",
    "Large Language Models are complex to build and expensive to train from scratch. Luckily there are pretrained LLMs available for use right away. [KerasNLP](https://keras.io/keras_nlp/) provides a large number of pre-trained checkpoints that allow you to experiment with SOTA models without needing to train them yourself.\n",
    "\n",
    "KerasNLP is a natural language processing library that supports users through their entire development cycle. KerasNLP offers both pretrained models and modularized building blocks, so developers could easily reuse pretrained models or stack their own LLM.\n",
    "\n",
    "In a nutshell, for generative LLM, KerasNLP offers:\n",
    "\n",
    "Pretrained models with generate() method, e.g., [keras_nlp.models.GPT2CausalLM](https://keras.io/api/keras_nlp/models/gpt2/gpt2_causal_lm#gpt2causallm-class) and [keras_nlp.models.OPTCausalLM](https://keras.io/api/keras_nlp/models/opt/opt_causal_lm#optcausallm-class).\n",
    "Sampler class that implements generation algorithms such as Top-K, Beam and contrastive search. These samplers can be used to generate text with custom models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load a pre-trained GPT-2 model and generate some text\n",
    "\n",
    "KerasNLP provides a number of pre-trained models, such as [Google Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) and [GPT-2](https://openai.com/research/better-language-models). You can see the list of models available in the [KerasNLP repository](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n",
    "\n",
    "It's very easy to load the GPT-2 model as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/vocab.json\n",
      "\u001b[1m1042301/1042301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step     \n",
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/merges.txt\n",
      "\u001b[1m456318/456318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 20:31:47.808080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:47.834280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:47.837564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:47.842021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:47.845138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:47.848308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:48.034600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:48.036368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:48.038005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-16 20:31:48.039606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/model.h5\n",
      "\u001b[1m497986112/497986112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# To speed up training and generation, we use preprocessor of length 128\n",
    "# instead of full length 1024.\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=128,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once the model is loaded, you can use it to generate some text right away. Run the cells below to give it a try. It's as simple as calling a single function generate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "Red hat is 不見知石清，山爲處漫紫繫。秋長埃非曲，空屋爲池河。\n",
      "TOTAL TIME ELAPSED: 0.28s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"Red hat is \", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "That Italian restaurant is called \"The Italian Restaurant\" and it's located in a very small, quiet neighborhood of Venice. It's not that you're not going to find it. But, if you're in a rush, you're going to have to go to a restaurant that's a few hundred meters away. It's a little less crowded than you think, but there's still a lot of traffic. It's a little less crowded than you thought.\n",
      "\n",
      "The Italian Restaurant is a little like a restaurant in that you're not just looking at a menu, but you're also getting a taste of what it is and what it's like to cook in Italy. And the restaurant is a place that's very much in the tradition of Italian food. It's very much a place of Italian food. It's very much Italian.\n",
      "\n",
      "I think you're probably thinking, 'Oh, I don't know what's in there. I don't know what's in there. I\n",
      "TOTAL TIME ELAPSED: 1.32s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice how much faster the second call is. This is because the computational graph is [XLA compiled](https://www.tensorflow.org/xla) in the 1st run and re-used in the 2nd behind the scenes.\n",
    "\n",
    "The quality of the generated text looks OK, but we can improve it via fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# More on the GPT-2 model from KerasNLP\n",
    "\n",
    "Next up, we will actually fine-tune the model to update its parameters, but before we do, let's take a look at the full set of tools we have to for working with for [GPT2](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/).\n",
    "\n",
    "The code of GPT2 can be found [here](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/). Conceptually the GPT2CausalLM can be hierarchically broken down into several modules in KerasNLP, all of which have a from_preset() function that loads a pretrained model:\n",
    "\n",
    "[keras_nlp.models.GPT2Tokenizer](https://keras.io/api/keras_nlp/models/gpt2/gpt2_tokenizer#gpt2tokenizer-class): The tokenizer used by GPT2 model, which is a [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt).\n",
    "[keras_nlp.models.GPT2CausalLMPreprocessor](https://keras.io/api/keras_nlp/models/gpt2/gpt2_causal_lm_preprocessor#gpt2causallmpreprocessor-class): the preprocessor used by GPT2 causal LM training. It does the tokenization along with other preprocessing works such as creating the label and appending the end token.\n",
    "[keras_nlp.models.GPT2Backbone](https://keras.io/api/keras_nlp/models/gpt2/gpt2_backbone#gpt2backbone-class): the GPT2 model, which is a stack of [keras_nlp.layers.TransformerDecoder](https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder#transformerdecoder-class). This is usually just referred as GPT2.\n",
    "[keras_nlp.models.GPT2CausalLM](https://keras.io/api/keras_nlp/models/gpt2/gpt2_causal_lm#gpt2causallm-class): wraps GPT2Backbone, it multiplies the output of GPT2Backbone by embedding matrix to generate logits over vocab tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Finetune on Reddit dataset\n",
    "\n",
    "Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one step further to finetune the model so that it generates text in a specific style, short or long, strict or casual. In this tutorial, we will use reddit dataset for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow_datasets==4.9.* -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-16 20:32:54.494519: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/user/tensorflow_datasets/reddit_tifu/short/1.1.2...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00, 11.22s/ url]\n",
      "Extraction completed...: 0 file [00:11, ? file/s]\n",
      "Dl Size...: 100%|██████████| 639/639 [00:11<00:00, 56.91 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00, 11.23s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset reddit_tifu downloaded and prepared to /home/user/tensorflow_datasets/reddit_tifu/short/1.1.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "reddit_ds = tfds.load(\"reddit_tifu\", split=\"train\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's take a look inside sample data from the reddit TensorFlow Dataset. There are two features:\n",
    "\n",
    "document: text of the post.\n",
    "title: the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"me and a friend decided to go to the beach last sunday. we loaded up and headed out. we were about half way there when i decided that i was not leaving till i had seafood. \\n\\nnow i'm not talking about red lobster. no friends i'm talking about a low country boil. i found the restaurant and got directions. i don't know if any of you have heard about the crab shack on tybee island but let me tell you it's worth it. \\n\\nwe arrived and was seated quickly. we decided to get a seafood sampler for two and split it. the waitress bought it out on separate platters for us. the amount of food was staggering. two types of crab, shrimp, mussels, crawfish, andouille sausage, red potatoes, and corn on the cob. i managed to finish it and some of my friends crawfish and mussels. it was a day to be a fat ass. we finished paid for our food and headed to the beach. \\n\\nfunny thing about seafood. it runs through me faster than a kenyan \\n\\nwe arrived and walked around a bit. it was about 45min since we arrived at the beach when i felt a rumble from the depths of my stomach. i ignored it i didn't want my stomach to ruin our fun. i pushed down the feeling and continued. about 15min later the feeling was back and stronger than before. again i ignored it and continued. 5min later it felt like a nuclear reactor had just exploded in my stomach. i started running. i yelled to my friend to hurry the fuck up. \\n\\nrunning in sand is extremely hard if you did not know this. we got in his car and i yelled at him to floor it. my stomach was screaming and if he didn't hurry i was gonna have this baby in his car and it wasn't gonna be pretty. after a few red lights and me screaming like a woman in labor we made it to the store. \\n\\ni practically tore his car door open and ran inside. i ran to the bathroom opened the door and barely got my pants down before the dam burst and a flood of shit poured from my ass. \\n\\ni finished up when i felt something wet on my ass. i rubbed it thinking it was back splash. no, mass was covered in the after math of me abusing the toilet. i grabbed all the paper towels i could and gave my self a whores bath right there. \\n\\ni sprayed the bathroom down with the air freshener and left. an elderly lady walked in quickly and closed the door. i was just about to walk away when i heard gag. instead of walking i ran. i got to the car and told him to get the hell out of there.\"\n",
      "b'liking seafood'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 20:33:24.515996: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for document, title in reddit_ds:\n",
    "    print(document.numpy())\n",
    "    print(title.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In our case, we are performing next word prediction in a language model, so we only need the 'document' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    reddit_ds.map(lambda document, _: document)\n",
    "    .batch(32)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now you can finetune the model using the familiar fit() function. Note that preprocessor will be automatically called inside fit method since GPT2CausalLM is a keras_nlp.models.Task instance.\n",
    "\n",
    "This step takes quite a bit of GPU memory and a long time if we were to train it all the way to a fully trained state. Here we just use part of the dataset for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 20:34:02.485713: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2023-10-16 20:34:32.928423: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'fusion_631', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m483/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m14s\u001b[0m 882ms/step - accuracy: 0.3191 - loss: 3.3629"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 20:41:45.923932: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-10-16 20:41:45.928251: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 883ms/step - accuracy: 0.3193 - loss: 3.3608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_core.src.callbacks.history.History at 0x7f5d6eb3ef70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "# Linearly decaying learning rate.\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-5,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After fine-tuning is finished, you can again generate text using the same generate() function. This time, the text will be closer to Reddit writing style, and the generated length will be close to our preset length in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "I like basketball, and i like to be the best basketball player in the world. so this is my first day of basketball practice, so i have some basketball practice, and i'm going to be playing with this kid. so i'm sitting on a bench with the bench on, and this is where my dad is sitting. i'm just sitting there watching the basketball, and i'm watching the game. my dad comes to the bench, and i'm just sitting there, and he starts talking. and he starts saying something about how his mom was going to have a baby, and i said that i didn't mean\n",
      "TOTAL TIME ELAPSED: 28.46s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Into the Sampling Method\n",
    "In KerasNLP, we offer a few sampling methods, e.g., contrastive search, Top-K and beam sampling. By default, our GPT2CausalLM uses Top-k search, but you can choose your own sampling method.\n",
    "\n",
    "Much like optimizer and activations, there are two ways to specify your custom sampler:\n",
    "\n",
    "Use a string identifier, such as \"greedy\", you are using the default configuration via this way.\n",
    "Pass a [keras_nlp.samplers.Sampler](https://keras.io/api/keras_nlp/samplers/samplers#sampler-class) instance, you can use custom configuration via this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For more details on KerasNLP Sampler class, you can check the code [here](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/samplers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Finetune on Chinese Poem Dataset\n",
    "\n",
    "We can also finetune GPT2 on non-English datasets. For readers knowing Chinese, this part illustrates how to fine-tune GPT2 on Chinese poem dataset to teach our model to become a poet!\n",
    "\n",
    "Because GPT2 uses byte-pair encoder, and the original pretraining dataset contains some Chinese characters, we can use the original vocab to finetune on Chinese dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'chinese-poetry'...\n",
      "remote: Enumerating objects: 7278, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 7278 (delta 27), reused 55 (delta 15), pack-reused 7195\u001b[K\n",
      "Receiving objects: 100% (7278/7278), 199.10 MiB | 40.07 MiB/s, done.\n",
      "Resolving deltas: 100% (5315/5315), done.\n",
      "Updating files: 100% (2285/2285), done.\n"
     ]
    }
   ],
   "source": [
    "# Load chinese poetry dataset.\n",
    "!git clone https://github.com/chinese-poetry/chinese-poetry.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load text from the json file. We only use《全唐诗》for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "poem_collection = []\n",
    "for file in os.listdir(\"chinese-poetry/全唐诗\"):\n",
    "    if \".json\" not in file or \"poet\" not in file:\n",
    "        continue\n",
    "    full_filename = \"%s/%s\" % (\"chinese-poetry/全唐诗\", file)\n",
    "    with open(full_filename, \"r\") as f:\n",
    "        content = json.load(f)\n",
    "        poem_collection.extend(content)\n",
    "\n",
    "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's take a look at sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可憐明月光，委曲照空牀。自是元無夢，更悲今夜長。\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Similar as Reddit example, we convert to TF dataset, and only use partial data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 20:43:23.275949: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2023-10-16 20:43:53.248367: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'fusion_677', 32 bytes spill stores, 32 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_150', 32 bytes spill stores, 32 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_642', 36 bytes spill stores, 36 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_271', 32 bytes spill stores, 32 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_392', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m483/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m7s\u001b[0m 424ms/step - accuracy: 0.2635 - loss: 2.5223"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 20:47:25.586265: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 425ms/step - accuracy: 0.2646 - loss: 2.5166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_core.src.callbacks.history.History at 0x7f5c203021c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
    "    .batch(16)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Running through the whole dataset takes long, only take `500` and run 1\n",
    "# epochs for demo purposes.\n",
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-4,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's check the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨夜雨疏风骤秦，萬里百書自江深。山山除長自花樹，長長河漏書居。\n"
     ]
    }
   ],
   "source": [
    "output = gpt2_lm.generate(\"昨夜雨疏风骤\", max_length=200)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
